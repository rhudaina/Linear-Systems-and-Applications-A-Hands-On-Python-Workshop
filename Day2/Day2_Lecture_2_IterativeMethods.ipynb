{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhudaina/Linear-Systems-and-Applications-A-Hands-On-Python-Workshop/blob/main/Day2/Day2_Lecture_2_IterativeMethods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "5RNGj9vUZwXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4rQwiYqnUQ1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj28JO9GnUQ0"
      },
      "source": [
        "# Sparse Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYo_wTvZnUQ3"
      },
      "source": [
        "A $n\\times m$ matrix is **sparse** if it has few non-zero entries in comparison to all $nm$ total entries.  Sparsity is a qualitative notion - it might mean we have $O(\\min\\{n,m\\})$ non-zero entries (for example, a diagonal matrix), it might also mean we have $O(nm)$ entries, but the constant is small (for example, $mn/100$).  A **dense** matrix is not sparse, meaning that most (or all) of the entries are non-zero.\n",
        "\n",
        "Recall the formula for matrix-vector multiplication:\n",
        "$$y_i = \\sum_j A_{i,j} x_j$$\n",
        "\n",
        "When we multiply a vector (or matrix) by a sparse matrix $A$, most of the coefficients are zero, and so we might expect that we can apply the matrix more quickly than we might apply a dense matrix.  We can rewrite the matrix-vector multiplication formula as\n",
        "$$y_i = \\sum_{j\\in nz(i)} A_{i,j} x_j$$\n",
        "\n",
        "Where $nz(i)$ denotes the column indices $j$ for which $A_{i,j}$ is non-zero.  We see the complexity of multiplying a sparse matrix is $O(nnz(A))$, where $nnz(A)$ is the number of non-zeros (note that when $A$ is dense, $nnz(A) = mn$)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse Matrix Formats"
      ],
      "metadata": {
        "id": "5c7gN_17okeA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeSfhGZqnUQ3"
      },
      "source": [
        "There are a variety of ways sparse matrices are stored in practice.  The utility of each format depends on whether there is any structure in the non-zeros, or what the matrix will be used for.\n",
        "\n",
        "Scipy provides several standard types of sparse matrices in `sicpy.sparse`.  See the [documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html#sparse-matrix-classes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COOrdinate Format"
      ],
      "metadata": {
        "id": "e2HFg83yooSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps the easiest to describe is the COO (COOrdinate format), which just stores three lists `i,j,data`, where `i[k]` and  `j[k]` are the row and column indices for a non-zero entry with value `data[k]`.\n",
        "\n",
        "Scipy documentation is [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix)"
      ],
      "metadata": {
        "id": "DNOiLmC3opkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNhf68vDnUQ3"
      },
      "outputs": [],
      "source": [
        "row  = [0,2,1]\n",
        "col  = [0,2,1]\n",
        "val  = [1,1,1]\n",
        "\n",
        "S = sp.sparse.coo_matrix((val, (row,col)), shape=(3,3))\n",
        "print(S)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sparse matrix formats have a `toarray` command which converts to a dense array."
      ],
      "metadata": {
        "id": "rUIAl8A4rVF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S.toarray()"
      ],
      "metadata": {
        "id": "tWBTNIyurcfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example:"
      ],
      "metadata": {
        "id": "Bo3NSR_jrhwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fegqmr5SnUQ4"
      },
      "outputs": [],
      "source": [
        "row  = [0,1,2,2]\n",
        "col  = [0,1,2,0]\n",
        "val  = [1,1,1,2]\n",
        "\n",
        "S = sp.sparse.coo_matrix((val, (row,col)), shape=(3,3))\n",
        "S.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NZ7AN6WnUQ4"
      },
      "source": [
        "You can visualize the sparsity pattern using PyPlot's `spy` function (this is particularly useful for large sparse matrices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB3Mkeu_nUQ4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.spy(S)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BitZZdvunUQ4"
      },
      "source": [
        "### Compressed Sparse Row/Column Formats\n",
        "\n",
        "One of the disadvantages of COO Matrices are that entries need not be ordered in any way, which can lead to inefficiencies in memory access in matrix-vector or matrix-matrix multiplication.\n",
        "\n",
        "Commonly used formats which keeps entries in a sensible order (without additional structure assumed) are Compressed Sparse Row (CSR) and Compressed Sparse Column (CSC) matrices.  You might think of these as the sparse equivalents of row-major and column-major dense matrices.\n",
        "\n",
        "We'll describe CSC matrices - the transpose is a CSR matrix.\n",
        "\n",
        "If `S` is a CSC matrix with `m` rows, `n` columns, and `nnz` non-zeros, we specify `S` with three lists: `ptr` (length `n+1`), `row` (length `nnz`) and `val` (length `nnz`).  The non-zero row indices for column `j` are stored in `row[ptr[j]:ptr[j+1]]`, and the non-zero values for those rows are stored in `val[ptr[j]:ptr[j+1]]`.  If you're familiar with pointers in a language like C, `ptr` is an array of pointer offsets.\n",
        "\n",
        "Basically, the non-zero entries for each column are stored in contiguous blocks of memory.  This can make it much faster to apply CSC matrices.\n",
        "\n",
        "Scipy documentation is [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5bSEHfFnUQ4"
      },
      "outputs": [],
      "source": [
        "ptr = [0,2,4,5]\n",
        "row = [0,2,0,1,2]\n",
        "val = [1,2,3,1,1]\n",
        "\n",
        "S = sp.sparse.csc_matrix((val, row, ptr), shape=(3,3))\n",
        "S.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use `toarray` command to get a numpy array without the matrix wrapper"
      ],
      "metadata": {
        "id": "PHlNdAL6rLNP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww29Mnc4nUQ4"
      },
      "outputs": [],
      "source": [
        "# the pointer list gives you slices to get the data for each column\n",
        "j = 1\n",
        "row[ptr[j]:ptr[j+1]], val[ptr[j]:ptr[j+1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3tnjp0unUQ5"
      },
      "source": [
        "### Other Sparse Matrix Types\n",
        "\n",
        "Other matrix types in `scipy.sparse` include:\n",
        "* [`dia_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dia_matrix.html#scipy.sparse.dia_matrix), which is good for diagonal/banded matrices\n",
        "* [`lil_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix), or a (row-based) list-of-lists matrix, which is good for mutating row operations\n",
        "* [`bsr_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.bsr_matrix.html#scipy.sparse.bsr_matrix), or block sparse row, which is good for sparse matrices with dense blocks\n",
        "* [`dok_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html#scipy.sparse.dok_matrix), or dictionary of keys, which is good for when you want to access and change individual entries quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5XVqjuynUQ5"
      },
      "outputs": [],
      "source": [
        "data = np.array([[1,2,3],[4,5,6]])\n",
        "S = sp.sparse.dia_matrix(\n",
        "  (data, [0,1]),\n",
        "  shape=(3,3))\n",
        "S.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lepUPYxnUQ5"
      },
      "outputs": [],
      "source": [
        "A = np.eye(5) # identity\n",
        "As = sp.sparse.dia_matrix(A)\n",
        "As"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wumQMEATnUQ6"
      },
      "source": [
        "To convert between sparse matrix formats, you can use `tocsr`, `tocoo`, etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "As1 = As.tocsr()\n",
        "As1"
      ],
      "metadata": {
        "id": "jZ6x-1CYvYVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvwIUnNinUQ6"
      },
      "outputs": [],
      "source": [
        "As2 = As.tocoo()\n",
        "As2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Sparse Matrices"
      ],
      "metadata": {
        "id": "J30LRLuQt3e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense matrices can be easily stored and read from comma-separated value formats using e.g. `np.loadtxt` and `np.savetxt`.  Because sparse matrices can be stored more efficiently than dense matrices, they have special storage formats.\n",
        "\n",
        "One source of sparse matrices which is used extensively for testing is the University of Florida Sparse Matrix Collection ([Link](https://sparse.tamu.edu/)).  As an example, we'll just read the `1138_bus.mtx` file, which is matrix-market format, and you can download from that link.  This is a plain text file, with a header (every line begins with `%`), and the first row contains three integers: the number of rows, number of columns, and number of nonzeros in the matrix.  For `1138_bus.mtx`, this looks like:\n",
        "```\n",
        "%%MatrixMarket matrix coordinate real symmetric\n",
        "%-------------------------------------------------------------------------------\n",
        "% UF Sparse Matrix Collection, Tim Davis\n",
        "% http://www.cise.ufl.edu/research/sparse/matrices/HB/1138_bus\n",
        "% name: HB/1138_bus\n",
        "% [S ADMITTANCE MATRIX 1138 BUS POWER SYSTEM, D.J.TYLAVSKY, JULY 1985.]\n",
        "% id: 1\n",
        "% date: 1985\n",
        "% author: D. Tylavsky\n",
        "% ed: I. Duff, R. Grimes, J. Lewis\n",
        "% fields: title A name id date author ed kind\n",
        "% kind: power network problem\n",
        "%-------------------------------------------------------------------------------\n",
        "1138 1138 2596\n",
        "```\n",
        "So the matrix is `1138 x 1138` with 2596 nonzeros.\n",
        "Every subsequent row is in the form `row, column, data` - one nonzero in COO format."
      ],
      "metadata": {
        "id": "zqaIVupPt7zP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1w76gL5seJT"
      },
      "source": [
        "Let's go ahead and load this matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9jdLYw2seJT"
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt('1138_bus.mtx', comments='%') # skip any rows that begin with `%`\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_pcJWhseJT"
      },
      "source": [
        "The first non-comment row contains the size of the matrix, so we can handle it separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd5H3wpyseJT"
      },
      "outputs": [],
      "source": [
        "m, n = int(data[0,0]), int(data[0,1])\n",
        "data = data[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOURACH9seJT"
      },
      "source": [
        "Matrix market format uses the Fortran convention of beginning indexes at 1, so we need to subtract 1 from indices to get the correct Python indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb9oh-mKseJT"
      },
      "outputs": [],
      "source": [
        "rows = data[:,0] - 1\n",
        "cols = data[:,1] - 1\n",
        "vals = data[:,2]\n",
        "A = sp.sparse.coo_matrix((vals, (rows, cols)), shape=(m,n))\n",
        "\n",
        "plt.spy(A)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12cXbSvaseJU"
      },
      "source": [
        "Let's look at the difference between using the sparse matrix and a dense matrix for matrix-vector multiplications:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "x = np.random.randn(n)\n",
        "y = np.empty_like(x)\n",
        "\n",
        "t_start = time.time()\n",
        "y = A @ x\n",
        "t_end = time.time()\n",
        "tcoo = t_end - t_start\n",
        "print(\"time for COO multiply: %f sec\" % tcoo)"
      ],
      "metadata": {
        "id": "PjhMP1uau8s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHbrVSzpseJU"
      },
      "outputs": [],
      "source": [
        "Adense = A.todense()\n",
        "\n",
        "t_start = time.time()\n",
        "y = Adense @ x\n",
        "t_end = time.time()\n",
        "tdense = t_end - t_start\n",
        "print(\"time for dense multiply: %f sec\\n\" % tdense)\n",
        "\n",
        "print(\"COO is %f times faster\" % (tdense / tcoo))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrQktdG2nUQ9"
      },
      "source": [
        "Using the sparse matrix is several times faster than using a dense matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Linear Algebra"
      ],
      "metadata": {
        "id": "0YUCLc5FwYyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have seen how sparse matrices and linear operators can be used to speed up basic matrix-vector and matrix-matrix operations, and decrease the memory footprint of the representation of a linear map.\n",
        "\n",
        "Just as there are special data types for sparse and structured matrices, there are specialized linear algebra routines which allow you to take advantage of sparsity and fast matrix-vector products.\n",
        "\n",
        "Routines for sparse linear algebra are found in `scipy.sparse.linalg`"
      ],
      "metadata": {
        "id": "eSp_aX5IwaZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 100\n",
        "A = sp.sparse.random(n, n, 0.01) + sp.sparse.eye(n)\n",
        "A"
      ],
      "metadata": {
        "id": "-4R0ERXjyNme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse Direct Methods"
      ],
      "metadata": {
        "id": "5h8SNZwzxYXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.spy(A)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WMHUpNmCx-CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This typically refers to producing a factorization of a sparse matrix for use in solving linear systems.\n",
        "\n",
        "The thing to keep in mind is that many factorizations will generally be dense, even if the original matrix is sparse, e.g. eigenvalue decompositions, QR decomposition, SVD, etc.  This means that if we compute a factorization, we are going to lose all the advantages we had from sparsity.  \n",
        "\n",
        "What we really want is a factorization where if `A` is sparse, the terms in the factorization are also sparse.  The factorization where this is easiest to achieve is the LU decomposition.  In general, the `L` and `U` terms will be more dense than `A`, and sometimes much more dense.  However, we can seek a permuted version of the matrix `A` which will minimize the amount of \"fill-in\" which occurs.  This is often done using \"nested disection\" algorithm, which is outside the scope of this course.  If you ever need to do this explicitly, the [METIS package](http://glaros.dtc.umn.edu/gkhome/metis/metis/overview) is commonly used.\n",
        "\n",
        "We'll just use the function [`scipy.linalg.splu`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.splu.html#scipy.sparse.linalg.splu) (SParse LU) at a high level, which produces a factorization object that can be used to solve linear systems."
      ],
      "metadata": {
        "id": "IJTW7O8-xZ5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = A.tocsc() # need to convert to CSC form first\n",
        "LU = sp.sparse.linalg.splu(A)\n",
        "LU"
      ],
      "metadata": {
        "id": "rXH1pcpKysNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting object stores the factors necessary to compute `A = PLUQ` (`P` permutes rows, and `Q` permutes columns).  It is computed using the [SuperLU library](https://portal.nersc.gov/project/sparse/superlu/).  Typically, you will just use the `solve` method on this object."
      ],
      "metadata": {
        "id": "bRmuzUs3y8YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(n)\n",
        "y = A @ x\n",
        "\n",
        "x2 = LU.solve(y)\n",
        "print(np.linalg.norm(x2 - x))"
      ],
      "metadata": {
        "id": "qSSARzyZy-22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the `scipy.sparse.linalg.spsolve` function, which wraps this factorization."
      ],
      "metadata": {
        "id": "UZcxpQSbzH1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = sp.sparse.linalg.spsolve(A, y)\n",
        "print(np.linalg.norm(x2 - x))"
      ],
      "metadata": {
        "id": "2CB8wDzLzMPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterative Methods"
      ],
      "metadata": {
        "id": "_E1eeaPPgS2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a large sparse random matrix $A$."
      ],
      "metadata": {
        "id": "7eeAEFvp1Vf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "A = sp.sparse.random(n, n, density=0.0001, format='csr') + sp.sparse.eye(n, format='csr') * n\n",
        "\n",
        "plt.spy(A)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pAVxicRS1kgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(n)\n",
        "y = A @ x"
      ],
      "metadata": {
        "id": "nNWOAh-c1-XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jacobi Iteration"
      ],
      "metadata": {
        "id": "wZvRudCI2jEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_jacobi(A, y, x0, TOL, maxIter):\n",
        "  for k in range(maxIter):\n",
        "    x = np.zeros(n)\n",
        "    for i in range(n):\n",
        "      x[i] = y[i] - sum(A[i,0:i]*x0[0:i]) - sum(A[i,i+1:n]*x0[i+1:n])\n",
        "      x[i] = x[i]/A[i,i]\n",
        "\n",
        "    if max(abs(x-x0))<TOL:\n",
        "      break\n",
        "\n",
        "    x0 = x\n",
        "\n",
        "  if k == maxIter-1:\n",
        "    print('\\nMaximum number of iterations reached!')\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "DFuIH7HA5AnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.ones(n)   # initial guess\n",
        "TOL = 1.0e-10     # tolerance\n",
        "maxIter = 1000    # maximum number of iterations\n",
        "Adense = A.toarray()\n",
        "\n",
        "t_start = time.time()\n",
        "x1 = solve_jacobi(Adense, y, x0, TOL, maxIter)\n",
        "t_end = time.time()\n",
        "\n",
        "print(\"elapsed time:\", t_end, \"sec\")\n",
        "print(np.linalg.norm(x1 - x))"
      ],
      "metadata": {
        "id": "Vti4_2t85Xl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse Iterative Methods"
      ],
      "metadata": {
        "id": "nM74nzbP0H8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sparse iterative methods are another class of methods you can use for solving linear systems built on [Krylov subspaces](https://en.wikipedia.org/wiki/Krylov_subspace).  They only require matrix-vector products, and are ideally used with sparse matrices and fast linear operators.  You can typically learn the theory behind these methods in a numerical linear algebra course - we'll just talk about how to use them.\n",
        "\n",
        "All these methods are meant to solve linear systems: find `x` so that `A @ x = b`, or least squares problems minimizing `norm(A @ x - b)`\n",
        "\n",
        "You can find a list of options in the [documentation for `scipy.sparse.linalg`](https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html#solving-linear-problems).  Here are some common options:\n",
        "\n",
        "* Conjugate Gradient: `scipy.sparse.linalg.cg` for `A` SPD\n",
        "* MINRES: `scipy.sparse.linalg.minres` for `A` symmetric\n",
        "* GMRES: `scipy.sparse.linalg.gmres` for general square `A`\n",
        "* LSQR: `scipy.sparse.linalg.lsqr` for solving least squares problems\n",
        "\n",
        "For example, we can use `gmres` with the same matrix we used for `splu`:"
      ],
      "metadata": {
        "id": "8Bs9FUGk0Jq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.empty_like(x)\n",
        "\n",
        "t_start = time.time()\n",
        "x2, exit = sp.sparse.linalg.gmres(A, y)\n",
        "t_end = time.time()\n",
        "print(\"GMRES:\", t_end - t_start, \"sec\")\n",
        "print(np.linalg.norm(x2 - x))"
      ],
      "metadata": {
        "id": "cTTvev3M7OqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_start = time.time()\n",
        "x3 = sp.sparse.linalg.spsolve(A, y)\n",
        "t_end = time.time()\n",
        "print(\"spsolve:\", t_end - t_start, \"sec\")\n",
        "print(np.linalg.norm(x3 - x))"
      ],
      "metadata": {
        "id": "tI3HYKmc6o7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "1.   [Brad Nelson (2021), Scientific Computing with Python](https://caam37830.github.io/book/index.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "pBsc-0E1Ar6K"
      }
    }
  ]
}